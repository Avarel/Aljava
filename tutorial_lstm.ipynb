{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tutorial-lstm",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Avarel/Aljava/blob/master/tutorial_lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXm8CSD9uvFB",
        "colab_type": "text"
      },
      "source": [
        "# Setup\n",
        "Importing **Tensorflow** and the necessities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sN23rYI_t_uC",
        "colab_type": "code",
        "outputId": "b6370031-19ce-41e2-b839-ef4ef240bf48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "!pip install tensorflow-gpu\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import datetime\n",
        "\n",
        "from tensorflow.python.client import device_lib"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-gpu in /usr/local/lib/python3.6/dist-packages (2.0.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /tensorflow-2.1.0/python3.6 (from tensorflow-gpu) (0.8.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.0.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /tensorflow-2.1.0/python3.6 (from tensorflow-gpu) (3.11.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /tensorflow-2.1.0/python3.6 (from tensorflow-gpu) (0.1.8)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /tensorflow-2.1.0/python3.6 (from tensorflow-gpu) (3.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /tensorflow-2.1.0/python3.6 (from tensorflow-gpu) (0.33.6)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /tensorflow-2.1.0/python3.6 (from tensorflow-gpu) (0.9.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /tensorflow-2.1.0/python3.6 (from tensorflow-gpu) (1.11.2)\n",
            "Requirement already satisfied: gast==0.2.2 in /tensorflow-2.1.0/python3.6 (from tensorflow-gpu) (0.2.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /tensorflow-2.1.0/python3.6 (from tensorflow-gpu) (1.13.0)\n",
            "Requirement already satisfied: tensorboard<2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.0.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /tensorflow-2.1.0/python3.6 (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /tensorflow-2.1.0/python3.6 (from tensorflow-gpu) (1.0.8)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /tensorflow-2.1.0/python3.6 (from tensorflow-gpu) (1.17.4)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /tensorflow-2.1.0/python3.6 (from tensorflow-gpu) (1.25.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /tensorflow-2.1.0/python3.6 (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: setuptools in /tensorflow-2.1.0/python3.6 (from protobuf>=3.6.1->tensorflow-gpu) (42.0.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /tensorflow-2.1.0/python3.6 (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (0.4.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /tensorflow-2.1.0/python3.6 (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (3.1.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /tensorflow-2.1.0/python3.6 (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (1.9.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /tensorflow-2.1.0/python3.6 (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (2.22.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /tensorflow-2.1.0/python3.6 (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (0.16.0)\n",
            "Requirement already satisfied: h5py in /tensorflow-2.1.0/python3.6 (from keras-applications>=1.0.8->tensorflow-gpu) (2.10.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /tensorflow-2.1.0/python3.6 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (1.3.0)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /tensorflow-2.1.0/python3.6 (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /tensorflow-2.1.0/python3.6 (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (0.2.7)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /tensorflow-2.1.0/python3.6 (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (3.1.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /tensorflow-2.1.0/python3.6 (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /tensorflow-2.1.0/python3.6 (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /tensorflow-2.1.0/python3.6 (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (2019.11.28)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /tensorflow-2.1.0/python3.6 (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (1.25.7)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /tensorflow-2.1.0/python3.6 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /tensorflow-2.1.0/python3.6 (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (0.4.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLcumD2UxrUg",
        "colab_type": "text"
      },
      "source": [
        "Checking the runtime and hardware."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhAiHom3uBwR",
        "colab_type": "code",
        "outputId": "c473edce-3330-4ed4-fa9e-d0d30be69579",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "source": [
        "print(device_lib.list_local_devices())\n",
        "print(\"TensorFlow version: \", tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 968094530466318055\n",
            ", name: \"/device:XLA_CPU:0\"\n",
            "device_type: \"XLA_CPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 10264808288884156612\n",
            "physical_device_desc: \"device: XLA_CPU device\"\n",
            ", name: \"/device:XLA_GPU:0\"\n",
            "device_type: \"XLA_GPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 2113702616245548143\n",
            "physical_device_desc: \"device: XLA_GPU device\"\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 15956161332\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 12908716877714422554\n",
            "physical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\"\n",
            "]\n",
            "TensorFlow version:  2.1.0-rc1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cb2M4_Zsulry",
        "colab_type": "code",
        "outputId": "04093cdb-2bea-4041-ccc6-d678b00214e2",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 397
        }
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "    print('User uploaded file {name} with size {length} bytes'.format(name = fn, length = len(uploaded[fn])))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ae00f101-7009-4528-9e9c-c2df817f11f4\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-ae00f101-7009-4528-9e9c-c2df817f11f4\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-761e87e5d4a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0muploaded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m     result = _output.eval_js(\n\u001b[1;32m     71\u001b[0m         'google.colab._files._uploadFilesContinue(\"{output_id}\")'.format(\n\u001b[0;32m---> 72\u001b[0;31m             output_id=output_id))\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'action'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'append'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m       \u001b[0;31m# JS side uses a generator of promises to process all of the files- some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiK9gIum0Gk0",
        "colab_type": "code",
        "outputId": "0eae6d32-f448-4a95-b23c-6bac8876bcdc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "path = os.getcwd()\n",
        "\n",
        "text = open(path + '/Iliad_v3.txt',  'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "print(\"Text is {} characters long\".format(len(text)))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text is 886809 characters long\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rw_LHmgy0WDf",
        "colab_type": "code",
        "outputId": "452c2e18-83b8-487f-a3e2-0baf5d2c45e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "words = [w for w in text.split(' ') if w.strip() != '' or w == '\\n']\n",
        "print(\"Text is {} words long\".format(len(words)))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text is 153260 words long\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciXSwK_P0fl-",
        "colab_type": "code",
        "outputId": "c9781130-1284-470b-f0ca-c6571d7e8bdb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(text[0:100])"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  achilles' wrath, to greece the direful spring\n",
            "  of woes unnumber'd, heavenly goddess, sing!\n",
            "  that\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fART55C10f-x",
        "colab_type": "code",
        "outputId": "94cc2216-009b-49f9-99f3-cf598743f8a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        }
      },
      "source": [
        "vocab = sorted(set(text))\n",
        "print ('There are {} unique characters'.format(len(vocab)))\n",
        "char2int = {c:i for i, c in enumerate(vocab)}\n",
        "int2char = np.array(vocab)\n",
        "print('Vector:\\n')\n",
        "for char,_ in zip(char2int, range(len(vocab))):\n",
        "    print(' {:4s}: {:3d},'.format(repr(char), char2int[char]))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 34 unique characters\n",
            "Vector:\n",
            "\n",
            " '\\n':   0,\n",
            " ' ' :   1,\n",
            " '!' :   2,\n",
            " \"'\" :   3,\n",
            " ',' :   4,\n",
            " '-' :   5,\n",
            " '.' :   6,\n",
            " '?' :   7,\n",
            " 'a' :   8,\n",
            " 'b' :   9,\n",
            " 'c' :  10,\n",
            " 'd' :  11,\n",
            " 'e' :  12,\n",
            " 'f' :  13,\n",
            " 'g' :  14,\n",
            " 'h' :  15,\n",
            " 'i' :  16,\n",
            " 'j' :  17,\n",
            " 'k' :  18,\n",
            " 'l' :  19,\n",
            " 'm' :  20,\n",
            " 'n' :  21,\n",
            " 'o' :  22,\n",
            " 'p' :  23,\n",
            " 'q' :  24,\n",
            " 'r' :  25,\n",
            " 's' :  26,\n",
            " 't' :  27,\n",
            " 'u' :  28,\n",
            " 'v' :  29,\n",
            " 'w' :  30,\n",
            " 'x' :  31,\n",
            " 'y' :  32,\n",
            " 'z' :  33,\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDN8_e-s2va7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "6b21450b-3702-482b-d84b-98bab9480154"
      },
      "source": [
        "text_as_int = np.array([char2int[ch] for ch in text], dtype=np.int32)\n",
        "print ('{}\\n mapped to integers:\\n {}'.format(repr(text[:100]), text_as_int[:100]))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"  achilles' wrath, to greece the direful spring\\n  of woes unnumber'd, heavenly goddess, sing!\\n  that\"\n",
            " mapped to integers:\n",
            " [ 1  1  8 10 15 16 19 19 12 26  3  1 30 25  8 27 15  4  1 27 22  1 14 25\n",
            " 12 12 10 12  1 27 15 12  1 11 16 25 12 13 28 19  1 26 23 25 16 21 14  0\n",
            "  1  1 22 13  1 30 22 12 26  1 28 21 21 28 20  9 12 25  3 11  4  1 15 12\n",
            "  8 29 12 21 19 32  1 14 22 11 11 12 26 26  4  1 26 16 21 14  2  0  1  1\n",
            " 27 15  8 27]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgtpwsV327li",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "43deed90-48c1-4c01-9345-0fc52b6a3f6e"
      },
      "source": [
        "tr_text = text_as_int[:704000] \n",
        "val_text = text_as_int[704000:] \n",
        "print(text_as_int.shape, tr_text.shape, val_text.shape)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(886809,) (704000,) (182809,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejr-r_jl2_n4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 64\n",
        "buffer_size = 10000\n",
        "embedding_dim = 256\n",
        "epochs = 50\n",
        "seq_length = 200\n",
        "examples_per_epoch = len(text)//seq_length\n",
        "#lr = 0.001 #will use default for Adam optimizer\n",
        "rnn_units = 1024\n",
        "vocab_size = len(vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhDDaU5m3Cum",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "64090a9f-e7af-4c14-fb39-b14e5d52deaf"
      },
      "source": [
        "tr_char_dataset = tf.data.Dataset.from_tensor_slices(tr_text)\n",
        "val_char_dataset = tf.data.Dataset.from_tensor_slices(val_text)\n",
        "tr_sequences = tr_char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "val_sequences = val_char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "tr_dataset = tr_sequences.map(split_input_target).shuffle(buffer_size).batch(batch_size, drop_remainder=True)\n",
        "val_dataset = val_sequences.map(split_input_target).shuffle(buffer_size).batch(batch_size, drop_remainder=True)\n",
        "print(tr_dataset, val_dataset)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<BatchDataset shapes: ((64, 200), (64, 200)), types: (tf.int32, tf.int32)> <BatchDataset shapes: ((64, 200), (64, 200)), types: (tf.int32, tf.int32)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhgyRb913G9B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "            batch_input_shape=[batch_size, None]\n",
        "        ),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "        tf.keras.layers.LSTM(rnn_units,\n",
        "            return_sequences=True,\n",
        "            stateful=True,\n",
        "            recurrent_initializer='glorot_uniform'),\n",
        "        tf.keras.layers.Dropout(0.2), \n",
        "        tf.keras.layers.LSTM(\n",
        "            rnn_units,\n",
        "            return_sequences=True,\n",
        "            stateful=True,\n",
        "            recurrent_initializer='glorot_uniform'\n",
        "        ),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "        tf.keras.layers.Dense(vocab_size)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "model = build_model(\n",
        "    vocab_size = len(vocab),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        "    batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4r3sIku32BX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "5f0a1d18-e07c-45fe-aa63-eb7bd9cde15c"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (64, None, 256)           8704      \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (64, None, 256)           0         \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (64, None, 1024)          5246976   \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (64, None, 1024)          0         \n",
            "_________________________________________________________________\n",
            "lstm_5 (LSTM)                (64, None, 1024)          8392704   \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (64, None, 1024)          0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (64, None, 34)            34850     \n",
            "=================================================================\n",
            "Total params: 13,683,234\n",
            "Trainable params: 13,683,234\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jwXNs1o35iP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0fc32dae-9359-47d9-9fcf-6d2e1f7fc253"
      },
      "source": [
        "for input_example_batch, target_example_batch in tr_dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape)\n",
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels,    logits, from_logits=True)\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Loss:      \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 200, 34)\n",
            "Loss:       3.5264518\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nxqZv223_Xm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "model.compile(optimizer=optimizer, loss=loss)\n",
        "patience = 10\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4kB9OwF4CCp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 969
        },
        "outputId": "c19d398e-0f9f-483b-ce95-c9d295b780bc"
      },
      "source": [
        "checkpoint_dir = './checkpoints'+ datetime.datetime.now().strftime(\"_%Y.%m.%d-%H:%M:%S\")\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)\n",
        "history = model.fit(tr_dataset, epochs=epochs, callbacks=[checkpoint_callback, early_stop] , validation_data=val_dataset)\n",
        "print (\"Training stopped as there was no improvement after {} epochs\".format(patience))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train for 54 steps, validate for 14 steps\n",
            "Epoch 1/50\n",
            "54/54 [==============================] - 17s 313ms/step - loss: 2.9956 - val_loss: 2.7580\n",
            "Epoch 2/50\n",
            "54/54 [==============================] - 15s 271ms/step - loss: 2.5608 - val_loss: 2.3077\n",
            "Epoch 3/50\n",
            "54/54 [==============================] - 15s 272ms/step - loss: 2.1699 - val_loss: 2.0232\n",
            "Epoch 4/50\n",
            "54/54 [==============================] - 15s 271ms/step - loss: 1.9238 - val_loss: 1.8122\n",
            "Epoch 5/50\n",
            "54/54 [==============================] - 15s 273ms/step - loss: 1.7339 - val_loss: 1.6564\n",
            "Epoch 6/50\n",
            "54/54 [==============================] - 15s 274ms/step - loss: 1.5877 - val_loss: 1.5372\n",
            "Epoch 7/50\n",
            "54/54 [==============================] - 15s 271ms/step - loss: 1.4797 - val_loss: 1.4497\n",
            "Epoch 8/50\n",
            "54/54 [==============================] - 15s 274ms/step - loss: 1.3982 - val_loss: 1.3929\n",
            "Epoch 9/50\n",
            "54/54 [==============================] - 15s 273ms/step - loss: 1.3360 - val_loss: 1.3499\n",
            "Epoch 10/50\n",
            "54/54 [==============================] - 15s 272ms/step - loss: 1.2876 - val_loss: 1.3177\n",
            "Epoch 11/50\n",
            "54/54 [==============================] - 15s 272ms/step - loss: 1.2463 - val_loss: 1.2914\n",
            "Epoch 12/50\n",
            "54/54 [==============================] - 15s 271ms/step - loss: 1.2089 - val_loss: 1.2744\n",
            "Epoch 13/50\n",
            "54/54 [==============================] - 15s 273ms/step - loss: 1.1773 - val_loss: 1.2586\n",
            "Epoch 14/50\n",
            "54/54 [==============================] - 15s 272ms/step - loss: 1.1469 - val_loss: 1.2526\n",
            "Epoch 15/50\n",
            "54/54 [==============================] - 15s 272ms/step - loss: 1.1162 - val_loss: 1.2400\n",
            "Epoch 16/50\n",
            "54/54 [==============================] - 15s 273ms/step - loss: 1.0871 - val_loss: 1.2407\n",
            "Epoch 17/50\n",
            "54/54 [==============================] - 15s 272ms/step - loss: 1.0576 - val_loss: 1.2385\n",
            "Epoch 18/50\n",
            "54/54 [==============================] - 15s 273ms/step - loss: 1.0261 - val_loss: 1.2398\n",
            "Epoch 19/50\n",
            "54/54 [==============================] - 15s 271ms/step - loss: 0.9975 - val_loss: 1.2469\n",
            "Epoch 20/50\n",
            "54/54 [==============================] - 15s 272ms/step - loss: 0.9650 - val_loss: 1.2543\n",
            "Epoch 21/50\n",
            "54/54 [==============================] - 15s 271ms/step - loss: 0.9368 - val_loss: 1.2620\n",
            "Epoch 22/50\n",
            "54/54 [==============================] - 15s 271ms/step - loss: 0.9029 - val_loss: 1.2764\n",
            "Epoch 23/50\n",
            "54/54 [==============================] - 15s 275ms/step - loss: 0.8696 - val_loss: 1.2924\n",
            "Epoch 24/50\n",
            "54/54 [==============================] - 15s 273ms/step - loss: 0.8358 - val_loss: 1.3130\n",
            "Epoch 25/50\n",
            "54/54 [==============================] - 15s 271ms/step - loss: 0.8009 - val_loss: 1.3335\n",
            "Epoch 26/50\n",
            "54/54 [==============================] - 15s 270ms/step - loss: 0.7715 - val_loss: 1.3497\n",
            "Epoch 27/50\n",
            "54/54 [==============================] - 15s 271ms/step - loss: 0.7402 - val_loss: 1.3832\n",
            "Training stopped as there was no improvement after 10 epochs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbTmthIH6A1e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "56d731b7-b327-4ccc-e479-3125073e15c7"
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir)) \n",
        "model.build(tf.TensorShape([1, None]))\n",
        "def generate_text(model, start_string):\n",
        "    \n",
        "    print('Generating with seed: \"' + start_string + '\"')\n",
        "  \n",
        "    num_generate = 1000\n",
        "    input_eval = [char2int[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "    text_generated = []\n",
        "    temperature = 1.0\n",
        "    model.reset_states()\n",
        "    for i in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions,      num_samples=1)[-1,0].numpy()\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "        text_generated.append(int2char[predicted_id])\n",
        "    return (start_string + ''.join(text_generated))\n",
        "print(generate_text(model, start_string=\"joy of gods\"))"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generating with seed: \"joy of gods\"\n",
            "joy of gods and woes \n",
            "  rush'd to my matrons, by my rashness rest \n",
            "  the unhappy warrior thus  hus pains \n",
            "  this lame was pured thee on the prize of men,\n",
            "  and whose bright crese for ever burder \n",
            "  with all his counsels, which i juptice still'd,\n",
            "  before my eyes, thy counsil both approve \n",
            "  the immortal races drink the reins he took,\n",
            "  and still he lays, and lustling tith the ground.\n",
            "  now grant to troy the generous warrior's fate \n",
            "  untire of mans, from man and offspring slain,\n",
            "  the ten of trojans, fat from aris fell'd \n",
            "  when the great pride the crowded sceptre raged,\n",
            "  his beacied troops the tears and waters stood \n",
            "  a faithful steed, that strong around his flight \n",
            "  unknown to priam's race succeeds they stand,\n",
            "  wake'd o'er the deeds abidst the falling sound.\n",
            "  his foe driven spears, their slaughter'd verge  with watery mountains bless'd \n",
            "  brace orous took, and trembling, swains from view,\n",
            "  on the shield trojans with reproach the dame,\n",
            "  or life, expirious! stop, to courage torn,\n",
            "  a veil'\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}